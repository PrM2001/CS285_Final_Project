# Use clipped double-Q learning (from TD3)
num_critic_networks: 2
target_critic_backup_type: min

exp_name: hsddp_test_0

# All these are the same as from the last problem...
base_config: sac

total_steps: 5000
random_steps: 100
training_starts: 200  

batch_size: 128
replay_buffer_capacity: 5000

discount: 0.99
use_soft_target_update: true
soft_target_update_rate: 0.01

actor_gradient_type: reparametrize
num_critic_updates: 1

use_entropy_bonus: true
temperature: 0.05